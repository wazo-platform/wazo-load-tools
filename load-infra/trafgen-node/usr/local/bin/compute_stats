#!/bin/bash

ts=$(date +%s);
TS=${1-$ts}
COUNTS_PATH=/opt/wda/counts
STATS_PATH=/opt/wda/stats/
OUTPUT=$STATS_PATH/$TS.csv

echo  "Start converting the log files into json"
for x in $(seq 1 50); do
    /opt/wda/converter $x $TS;
done


echo  "Extract data per keys"
KEYS=$(echo "User_info Activities Dird_sources Rooms Contacts_statuses App_access Contacts Switchboard_fetched error")
for K in $KEYS; do
    for x in $(seq 1 50); do
        export JSON=$(cat $STATS_PATH/$TS/$x.json) && export KEY=$K;
        if jq  'has("$KEY")' <<< $JSON; then
            jq -r ."$KEY" $STATS_PATH/$TS/$x.json |grep -v null >>$COUNTS_PATH/$TS.$KEY.count;
        fi ;
    done
done



echo "Transform data as stats per keys"
for K in $KEYS; do
    if [ "$K" = "error" ]; then
	    continue
    fi
    /usr/local/bin/pd.py $COUNTS_PATH/$TS.$K.count > $COUNTS_PATH/$TS.$K.percentiles



    # generate  csv files
    mapfile -t lines < $COUNTS_PATH/$TS.$K.percentiles

    count=$(echo "${lines[1]}" | awk '{print $2}')
    mean=$(echo "${lines[2]}" | awk '{print $2}')
    std=$(echo "${lines[3]}" | awk '{print $2}')
    min=$(echo "${lines[4]}" | awk '{print $2}')
    q1=$(echo "${lines[5]}" | awk '{print $2}')
    median=$(echo "${lines[6]}" | awk '{print $2}')
    q3=$(echo "${lines[7]}" | awk '{print $2}')
    q9=$(echo "${lines[8]}" | awk '{print $2}')
    q99=$(echo "${lines[9]}" | awk '{print $2}')
    max=$(echo "${lines[10]}" | awk '{print $2}')

    echo "$K,$count,$mean,$std,$min,$q1,$median,$q3,$q9,$q99,$max" >> $OUTPUT
done

ERROR_COUNT=$(grep -ci error  $COUNTS_PATH/$TS.error.count)
echo "error,$ERROR_COUNT,,,,,,,," >> $OUTPUT

# CSV schema key,count,mean,std,min,q1,median,q3,q9,q99,max
